{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620ab3c6",
   "metadata": {},
   "source": [
    "# WFST with Neural LCS Alignment\n",
    "\n",
    "This notebook integrates the Neural LCS alignment functionality from the Neural-LCS project with WFST decoding for dysfluency detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd57078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import json\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2PhonemeCTCTokenizer, PreTrainedTokenizerFast, T5EncoderModel, T5Config\n",
    "from IPython.display import Audio, display\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "415c075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMU Phoneme to ID mapping\n",
    "phoneme_to_id = {\n",
    "    \"AA\": 0, \"AE\": 1, \"AH\": 2, \"AO\": 3, \"AW\": 4, \"AY\": 5,\n",
    "    \"B\": 6, \"CH\": 7, \"D\": 8, \"DH\": 9, \"EH\": 10, \"ER\": 11,\n",
    "    \"EY\": 12, \"F\": 13, \"G\": 14, \"HH\": 15, \"IH\": 16, \"IY\": 17,\n",
    "    \"JH\": 18, \"K\": 19, \"L\": 20, \"M\": 21, \"N\": 22, \"NG\": 23,\n",
    "    \"OW\": 24, \"OY\": 25, \"P\": 26, \"R\": 27, \"S\": 28, \"SH\": 29,\n",
    "    \"T\": 30, \"TH\": 31, \"UH\": 32, \"UW\": 33, \"V\": 34, \"W\": 35,\n",
    "    \"Y\": 36, \"Z\": 37, \"ZH\": 38,\n",
    "    \"<pad>\": 39, \"<unk>\": 40, \"<cls>\": 41, \"<sep>\": 42\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c79678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Phoneme Tokenizer\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class PhonemeTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, phoneme_to_id, **kwargs):\n",
    "        self.phoneme_to_id = phoneme_to_id\n",
    "        super().__init__(**kwargs)\n",
    "        self.id_to_phoneme = {v: k for k, v in phoneme_to_id.items()}\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.cls_token = \"<cls>\"\n",
    "        self.sep_token = \"<sep>\"\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        # return vocabulary table\n",
    "        return self.phoneme_to_id\n",
    "        \n",
    "    def _convert_token_to_id(self, token):\n",
    "        # Convert a single phoneme to its ID\n",
    "        return self.phoneme_to_id.get(token, self.phoneme_to_id.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        # Convert a single ID back to a phoneme\n",
    "        return self.id_to_phoneme.get(index, self.unk_token)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # Split text into phonemes and map them to IDs\n",
    "        return [self.phoneme_to_id.get(phoneme, self.phoneme_to_id.get(self.unk_token)) for phoneme in text.split()]\n",
    "\n",
    "    def encode(self, text, max_length = 120, add_special_tokens = True, padding = True):\n",
    "        max_len = max_length\n",
    "        token_ids = self._tokenize(text)\n",
    "        if add_special_tokens:\n",
    "            token_ids = token_ids + [self.phoneme_to_id[self.sep_token]]\n",
    "        if padding:\n",
    "            prev_len = len(token_ids)\n",
    "            token_ids = token_ids + [self.phoneme_to_id[self.pad_token]] * (max_len - prev_len)\n",
    "            mask = [1] * prev_len + [0] * (max_len - prev_len)\n",
    "        if padding:\n",
    "            return {\"input_ids\": torch.tensor(token_ids), \"attention_mask\": torch.tensor(mask)}\n",
    "        else:\n",
    "            return {\"input_ids\": torch.tensor(token_ids), \"attention_mask\": None}\n",
    "                \n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        tokens = [self.id_to_phoneme[token_id] for token_id in token_ids if token_id in self.id_to_phoneme]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in [self.pad_token, self.cls_token, self.sep_token]]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phoneme_to_id)\n",
    "\n",
    "tokenizer = PhonemeTokenizer(phoneme_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a4750e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard LCS Algorithm\n",
    "def find_lcs_new(seq1, seq2):\n",
    "    \"\"\"Find the longest common subsequence between two sequences using embedding distance.\"\"\"\n",
    "    ## seq1 is target\n",
    "    ## seq2 is source\n",
    "    \n",
    "    lengths = [[0] * (len(seq2) + 1) for _ in range(len(seq1) + 1)]\n",
    "    for i, x in enumerate(seq1):\n",
    "        for j, y in enumerate(seq2):\n",
    "            if  x == y:  # Adjust the threshold as needed\n",
    "                ### Similar\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                ### Not Similar\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    \n",
    "    # Reconstruct the LCS\n",
    "    align_lcs_result = [] #tuple (src, target)\n",
    "    x, y = len(seq1), len(seq2)\n",
    "    \n",
    "    \n",
    "    while x != 0 and y != 0:\n",
    "\n",
    "        if lengths[x][y] == lengths[x-1][y]:\n",
    "            x -= 1\n",
    "        elif lengths[x][y] == lengths[x][y-1]:\n",
    "            align_lcs_result.append((seq2[y - 1], seq1[x - 1]))\n",
    "            y -= 1\n",
    "        else:\n",
    "            align_lcs_result.append((seq2[y - 1], seq1[x - 1]))\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "            \n",
    "        if x == 0 and y != 0:\n",
    "            while y > 0: \n",
    "                align_lcs_result.append((seq2[y - 1], seq1[0]))\n",
    "                y -= 1\n",
    "            break\n",
    "        if x != 0 and y == 0:\n",
    "            align_lcs_result.append((seq2[0], seq1[x - 1]))\n",
    "            break\n",
    "    \n",
    "    align_lcs_result.reverse()\n",
    "\n",
    "    return align_lcs_result\n",
    "\n",
    "def insert_missing_tuples(ref_words, align_result):\n",
    "    # Extract the second elements from the align_result tuples\n",
    "    align_words = []\n",
    "    for t in align_result:\n",
    "        if len(align_words) == 0:\n",
    "            align_words.append(t[1])\n",
    "        elif align_words[-1] != t[1]:\n",
    "            align_words.append(t[1])\n",
    "            \n",
    "    missing_flg = [1] * len(ref_words)\n",
    "\n",
    "    for i, word in enumerate(ref_words):\n",
    "        missing_num = sum(missing_flg[:i])\n",
    "        j = i - missing_num\n",
    "        if j <len(align_words) and align_words[j] == word:\n",
    "            missing_flg[i] = 0\n",
    "\n",
    "    # Create a copy of align_result to insert missing words\n",
    "    new_align_result = []\n",
    "    ref_idx = 0  # Index to track the current position in ref_words\n",
    "\n",
    "    for idx, word in enumerate(ref_words):\n",
    "        # Insert missing words\n",
    "        if missing_flg[idx] == 1:\n",
    "            new_align_result.append((None, word))\n",
    "        # Add the next word from align_result if it matches\n",
    "        while ref_idx < len(align_result) and align_result[ref_idx][1] == word:\n",
    "            new_align_result.append(align_result[ref_idx])\n",
    "            ref_idx += 1\n",
    "\n",
    "    return new_align_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ad0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural LCS Model Architecture\n",
    "class PhonemeBoundaryAlignerT5(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"t5-small\", phoneme_vocab_size=50, gru_hidden_dim = 128, num_filters = 16, hidden_dim=512):\n",
    "        super(PhonemeBoundaryAlignerT5, self).__init__()\n",
    "        # Load pretrained T5 model\n",
    "        self.encoder = T5EncoderModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # Resize token embeddings to fit phoneme vocab size\n",
    "        self.encoder.resize_token_embeddings(phoneme_vocab_size)\n",
    "\n",
    "        # GRU Layer to capture long sentence feature\n",
    "        self.gru = nn.GRU(input_size = hidden_dim * 2, hidden_size = gru_hidden_dim, \n",
    "                          num_layers = 1, batch_first = True, bidirectional = True)\n",
    "\n",
    "        # 1D CNN Layer\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels = gru_hidden_dim * 2,\n",
    "            out_channels = num_filters,\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 1\n",
    "        )\n",
    "                \n",
    "        # Boundary Predictor: Fully connected layers\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_filters, num_filters//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_filters//2, 4),\n",
    "        )\n",
    "        \n",
    "        # Re-initialize weights if needed\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, ref, src, ref_mask=None, src_mask=None):\n",
    "        # Encode reference and source phonemes using T5\n",
    "        ref_output = self.encoder(input_ids=ref, attention_mask=ref_mask).last_hidden_state  # (batch_size, src_len, hidden_dim)\n",
    "        src_output = self.encoder(input_ids=src, attention_mask=src_mask).last_hidden_state  # (batch_size, src_len, hidden_dim)\n",
    "        \n",
    "        # Combine ref and src features\n",
    "        alignment_features = torch.cat((ref_output, src_output), dim=-1)  # (batch_size, src_len, hidden_dim * 2)\n",
    "        alignment_features, _ = self.gru(alignment_features)              # (batch_size, src_len, gru_hidden_dim * 2)\n",
    "\n",
    "        alignment_features = alignment_features.permute(0, 2, 1)          # (batch_size, gru_hidden_dim * 2, src_len)\n",
    "        \n",
    "        # Process with Conv layer to capture contextual information\n",
    "        conv_features = self.conv1d(alignment_features)     # (batch_size, num_filters, src_len) \n",
    "        conv_features = conv_features.permute(0, 2, 1)      # (batch_size, src_len, num_filters)\n",
    "        \n",
    "        # Through MLP layer\n",
    "        mlp_features = self.MLP(conv_features) # (batch_size, src_len, 3)\n",
    "        \n",
    "        return mlp_features\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Reinitialize model parameters for layers other than embeddings.\n",
    "        \"\"\"\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, torch.nn.Conv1d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, torch.nn.Embedding):\n",
    "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # Apply custom weight initialization to layers\n",
    "        self.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7513172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment Correction Function\n",
    "def correct(align_result, ref, src):\n",
    "    if align_result[-1][1] == ref[-1] and align_result[-1][0] == src[-1]: return align_result\n",
    "    elif align_result[-1][1] == align_result[-1][0]: return align_result\n",
    "    else:\n",
    "        mark = 0\n",
    "        split_bd = None\n",
    "        split_sign = None\n",
    "        idx = len(align_result) - 1\n",
    "        while idx > 0:\n",
    "            if align_result[idx][0] == align_result[idx][1]: \n",
    "                mark += 1\n",
    "            if mark == 2:\n",
    "                j = idx - 1\n",
    "                while j >= 0 and align_result[j][1] == align_result[idx][1]:\n",
    "                    j -= 1\n",
    "                split_bd = align_result[j][1]\n",
    "                split_sign = j\n",
    "                break\n",
    "            idx -= 1 \n",
    "        # print(split_bd)\n",
    "        if mark != 2 or split_bd == None: return align_result\n",
    "\n",
    "        align_right = deepcopy(align_result)\n",
    "        \n",
    "        idx = len(align_result) - 1\n",
    "        new_src = []\n",
    "        new_ref = []\n",
    "        while idx >= 0 and idx != split_sign:\n",
    "            align_right.pop()\n",
    "            if align_result[idx][0] != None:\n",
    "                new_src.insert(0, align_result[idx][0])\n",
    "            idx -= 1\n",
    "        \n",
    "        idx = len(align_result) - 1\n",
    "        while idx > 0 and idx != split_sign:\n",
    "            if align_result[idx][1] == None:\n",
    "                idx -= 1\n",
    "                continue\n",
    "            jdx = idx\n",
    "            while jdx >= 0 and align_result[jdx][1] == align_result[idx][1]:\n",
    "                jdx -= 1\n",
    "            jdx += 1\n",
    "            idx = jdx\n",
    "            new_ref.insert(0, align_result[jdx][1])\n",
    "            idx -= 1\n",
    "\n",
    "        # print(new_ref, new_src)\n",
    "\n",
    "        new_result = align_right + insert_missing_tuples(new_ref, find_lcs_new(new_ref, new_src))\n",
    "        return new_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d7a504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural LCS Function\n",
    "def neuralLCS(seq1, seq2, model):\n",
    "    ## seq1 is target\n",
    "    ## seq2 is source\n",
    "    tokenizer = PhonemeTokenizer(phoneme_to_id)\n",
    "    ref = tokenizer.encode(\" \".join(seq1), max_length = 120)[\"input_ids\"]\n",
    "    src = tokenizer.encode(\" \".join(seq2), max_length = 120)[\"input_ids\"]\n",
    "    ref_mask = tokenizer.encode(\" \".join(seq1), max_length = 120)[\"attention_mask\"]\n",
    "    src_mask = tokenizer.encode(\" \".join(seq2), max_length = 120)[\"attention_mask\"]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ref = torch.tensor(ref).unsqueeze(0).to(device)\n",
    "    src = torch.tensor(src).unsqueeze(0).to(device)\n",
    "    ref_mask = torch.tensor(ref_mask).unsqueeze(0).to(device)\n",
    "    src_mask = torch.tensor(src_mask).unsqueeze(0).to(device)\n",
    "\n",
    "    output = model(ref, src, ref_mask, src_mask).squeeze(0)\n",
    "    predicted_classes = torch.argmax(output, dim=1).tolist()\n",
    "\n",
    "    prediction = predicted_classes[: predicted_classes.index(3) + 1]\n",
    "    # prediction = prediction[: -1]\n",
    "    # print(prediction)\n",
    "\n",
    "    aln_mark = 0\n",
    "    src_mark = 0\n",
    "    align_result = []\n",
    "    left = None\n",
    "    for jdx, target in enumerate(seq1):\n",
    "        if prediction[aln_mark] == 3: \n",
    "            left = list(range(jdx, len(seq1)))\n",
    "            break\n",
    "        elif prediction[aln_mark] == 2: \n",
    "            align_result.append((None, target))\n",
    "            aln_mark += 1\n",
    "            continue\n",
    "        elif prediction[aln_mark] == 0:\n",
    "            i = aln_mark\n",
    "            while True:\n",
    "                if prediction[i] == 0 and src_mark < len(seq2): \n",
    "                    align_result.append((seq2[src_mark], target))\n",
    "                    i += 1\n",
    "                    src_mark += 1\n",
    "                else: break\n",
    "            aln_mark = i\n",
    "        \n",
    "        if prediction[aln_mark] == 1:\n",
    "            if src_mark < len(seq2):\n",
    "                align_result.append((seq2[src_mark], target))\n",
    "                aln_mark += 1\n",
    "                src_mark += 1\n",
    "\n",
    "    if left == None and jdx < len(seq1) - 1:\n",
    "        left = list(range(jdx, len(seq1)))\n",
    "\n",
    "    if left != None:\n",
    "        for item in left: align_result.append((None, seq1[item]))\n",
    "\n",
    "    if src_mark - 1 < len(seq2) - 1:\n",
    "        for item in list(range(src_mark, len(seq2))): align_result.append((seq2[src_mark], None))\n",
    "\n",
    "    return correct(align_result, seq1, seq2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badafc7e",
   "metadata": {},
   "source": [
    "# Load Neural LCS Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "038ed302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda\n",
      "Model parameters: 19,802,044\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Neural LCS model\n",
    "model_path = \"Neural-LCS/phn_lcs/model/phn_align_1.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PhonemeBoundaryAlignerT5(phoneme_vocab_size=len(tokenizer))\n",
    "model.load_state_dict(torch.load(model_path, map_location = device))\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded on device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b5b72",
   "metadata": {},
   "source": [
    "# Example Usage: Neural LCS Alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43c76d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sequence: ['EH', 'V', 'R', 'IY', 'W', 'AH', 'N', 'IH', 'Z', 'T', 'UW', 'AH', 'P', 'S', 'EH', 'T', 'UW', 'K', 'AA', 'M', 'EH', 'N', 'T']\n",
      "Source sequence: ['EH', 'V', 'R', 'IY', 'AE', 'N', 'IH', 'Z', 'T', 'UW', 'AH', 'P', 'S', 'EH', 'D', 'T', 'UW', 'K', 'AA', 'M', 'EH', 'N', 'T']\n",
      "\n",
      "Hard LCS alignment:\n",
      " 0: EH  -> EH \n",
      " 1: V   -> V  \n",
      " 2: R   -> R  \n",
      " 3: IY  -> IY \n",
      " 4: AE  -> IY \n",
      " 5: N   -> N  \n",
      " 6: IH  -> IH \n",
      " 7: Z   -> Z  \n",
      " 8: T   -> T  \n",
      " 9: UW  -> UW \n",
      "10: AH  -> AH \n",
      "11: P   -> P  \n",
      "12: S   -> S  \n",
      "13: EH  -> EH \n",
      "14: D   -> EH \n",
      "15: T   -> T  \n",
      "16: UW  -> UW \n",
      "17: K   -> K  \n",
      "18: AA  -> AA \n",
      "19: M   -> M  \n",
      "20: EH  -> EH \n",
      "21: N   -> N  \n",
      "22: T   -> T  \n",
      "\n",
      "Neural LCS alignment:\n",
      " 0: EH  -> EH \n",
      " 1: V   -> V  \n",
      " 2: R   -> R  \n",
      " 3: IY  -> IY \n",
      " 4: AE  -> IY \n",
      " 5: None -> W  \n",
      " 6: None -> AH \n",
      " 7: N   -> N  \n",
      " 8: IH  -> IH \n",
      " 9: Z   -> Z  \n",
      "10: T   -> T  \n",
      "11: UW  -> UW \n",
      "12: AH  -> AH \n",
      "13: P   -> P  \n",
      "14: S   -> S  \n",
      "15: EH  -> EH \n",
      "16: D   -> EH \n",
      "17: T   -> T  \n",
      "18: UW  -> UW \n",
      "19: K   -> K  \n",
      "20: AA  -> AA \n",
      "21: M   -> M  \n",
      "22: EH  -> EH \n",
      "23: N   -> N  \n",
      "24: T   -> T  \n"
     ]
    }
   ],
   "source": [
    "# Example: Compare Hard LCS vs Neural LCS\n",
    "seq1 = 'EH V R IY W AH N IH Z T UW AH P S EH T UW K AA M EH N T'.split(\" \")\n",
    "seq2 = 'EH V R IY AE N IH Z T UW AH P S EH D T UW K AA M EH N T'.split(\" \")\n",
    "\n",
    "print(\"Reference sequence:\", seq1)\n",
    "print(\"Source sequence:\", seq2)\n",
    "print()\n",
    "\n",
    "# Hard LCS alignment\n",
    "hard_align = find_lcs_new(seq1, seq2)\n",
    "print(\"Hard LCS alignment:\")\n",
    "for i, (src, ref) in enumerate(hard_align):\n",
    "    src_str = str(src) if src is not None else \"None\"\n",
    "    ref_str = str(ref) if ref is not None else \"None\"\n",
    "    print(f\"{i:2d}: {src_str:3s} -> {ref_str:3s}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Neural LCS alignment\n",
    "neural_align = neuralLCS(seq1, seq2, model)\n",
    "print(\"Neural LCS alignment:\")\n",
    "for i, (src, ref) in enumerate(neural_align):\n",
    "    src_str = str(src) if src is not None else \"None\"\n",
    "    ref_str = str(ref) if ref is not None else \"None\"\n",
    "    print(f\"{i:2d}: {src_str:3s} -> {ref_str:3s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2392129",
   "metadata": {},
   "source": [
    "# Integration with WFST Decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc861a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced WFST Decoding with Neural LCS Alignment\n",
    "def enhanced_wfst_decode_with_neural_lcs(audio_file, ref_text_file, use_neural_lcs=True):\n",
    "    \"\"\"\n",
    "    Enhanced WFST decoding that integrates Neural LCS alignment for better dysfluency detection.\n",
    "    \n",
    "    Args:\n",
    "        audio_file: Path to audio file\n",
    "        ref_text_file: Path to reference text file\n",
    "        use_neural_lcs: Whether to use Neural LCS alignment (True) or Hard LCS (False)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing decoding results and alignment information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load audio and text\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    with open(ref_text_file, 'r') as f:\n",
    "        ref_text = f.read().strip()\n",
    "    \n",
    "    print(f\"Processing: {audio_file}\")\n",
    "    print(f\"Reference text: {ref_text}\")\n",
    "    print(f\"Sample rate: {sample_rate}\")\n",
    "    \n",
    "    # Load Wav2Vec2 model\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-xlsr-53-phon-cv-ft\")\n",
    "    wav2vec_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-xlsr-53-phon-cv-ft\")\n",
    "    device_wav2vec = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Resample audio\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    waveform = waveform.squeeze()\n",
    "    input_values = processor(waveform, return_tensors=\"pt\", device=device_wav2vec, sampling_rate=16000).input_values\n",
    "    \n",
    "    # Get logits from Wav2Vec2\n",
    "    with torch.no_grad():\n",
    "        logits = wav2vec_model(input_values).logits\n",
    "        print(f'Logits shape: {logits.shape}')\n",
    "    \n",
    "    # WFST Decoding (assuming you have the WFST decoder from main.ipynb)\n",
    "    # This is a placeholder - you would need to import your WFST decoder\n",
    "    # from utils.fst import WFSTdecoder\n",
    "    # from utils.wper import W_PER\n",
    "    \n",
    "    # For now, we'll simulate the WFST decoding result\n",
    "    # In practice, you would use your actual WFST decoder here\n",
    "    simulated_result = {\n",
    "        'id': 'example',\n",
    "        'ref_phonemes': ['ð', 'e', 'l', 'l', 'ɛ', 'f', 't', 'ɚ', 'l', 'i'],\n",
    "        'dys_detect': [\n",
    "            {'phoneme': 'l', 'dysfluency_type': 'repetition'},\n",
    "            {'phoneme': 'l', 'dysfluency_type': 'normal'},\n",
    "            {'phoneme': 'ɛ', 'dysfluency_type': 'normal'},\n",
    "            {'phoneme': 'f', 'dysfluency_type': 'normal'},\n",
    "            {'phoneme': 't', 'dysfluency_type': 'normal'},\n",
    "            {'phoneme': 'ɚ', 'dysfluency_type': 'normal'},\n",
    "            {'phoneme': 'l', 'dysfluency_type': 'normal'},\n",
    "            {'phoneme': 'i', 'dysfluency_type': 'normal'}\n",
    "        ],\n",
    "        'decode_phonemes': ['ð', 'e', 'l', 'l', 'ɛ', 'f', 't', 'ɚ', 'l', 'i'],\n",
    "        'lattice': []\n",
    "    }\n",
    "    \n",
    "    # Convert IPA to CMU for alignment\n",
    "    ipa2cmu = json.load(open('config/ipa2cmu.json', 'r')) if os.path.exists('config/ipa2cmu.json') else {}\n",
    "    \n",
    "    def ipa_to_cmu(ipa_list):\n",
    "        cmu_list = []\n",
    "        for ipa in ipa_list:\n",
    "            if ipa in ipa2cmu:\n",
    "                cmu_value = ipa2cmu[ipa].split()[0]\n",
    "                cmu_list.append(cmu_value)\n",
    "            else:\n",
    "                cmu_list.append(ipa)  # fallback to original\n",
    "        return cmu_list\n",
    "    \n",
    "    ref_cmu = ipa_to_cmu(simulated_result['ref_phonemes'])\n",
    "    decode_cmu = ipa_to_cmu(simulated_result['decode_phonemes'])\n",
    "    \n",
    "    print(f\"Reference CMU: {ref_cmu}\")\n",
    "    print(f\"Decoded CMU: {decode_cmu}\")\n",
    "    \n",
    "    # Apply Neural LCS or Hard LCS alignment\n",
    "    if use_neural_lcs:\n",
    "        print(\"\\\\nUsing Neural LCS alignment...\")\n",
    "        alignment_result = neuralLCS(ref_cmu, decode_cmu, model)\n",
    "        alignment_type = \"Neural LCS\"\n",
    "    else:\n",
    "        print(\"\\\\nUsing Hard LCS alignment...\")\n",
    "        alignment_result = find_lcs_new(ref_cmu, decode_cmu)\n",
    "        alignment_type = \"Hard LCS\"\n",
    "    \n",
    "    print(f\"\\\\n{alignment_type} alignment result:\")\n",
    "    for i, (src, ref) in enumerate(alignment_result):\n",
    "        src_str = str(src) if src is not None else \"None\"\n",
    "        ref_str = str(ref) if ref is not None else \"None\"\n",
    "        print(f\"{i:2d}: {src_str:3s} -> {ref_str:3s}\")\n",
    "    \n",
    "    # Enhanced result with alignment information\n",
    "    enhanced_result = {\n",
    "        'original_result': simulated_result,\n",
    "        'alignment_type': alignment_type,\n",
    "        'alignment_result': alignment_result,\n",
    "        'ref_cmu': ref_cmu,\n",
    "        'decode_cmu': decode_cmu\n",
    "    }\n",
    "    \n",
    "    return enhanced_result\n",
    "\n",
    "# Example usage\n",
    "# result = enhanced_wfst_decode_with_neural_lcs(\"data/audio/p088_4067.wav\", \"data/gt_text/p088_4067.txt\", use_neural_lcs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a22b9a",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook integrates the Neural LCS alignment functionality from the Neural-LCS project with WFST decoding for enhanced dysfluency detection. The key components include:\n",
    "\n",
    "1. **Hard LCS Algorithm**: Traditional longest common subsequence alignment\n",
    "2. **Neural LCS Model**: T5-based neural alignment model for improved accuracy\n",
    "3. **Phoneme Tokenizer**: Custom tokenizer for CMU phoneme sequences\n",
    "4. **Enhanced WFST Decoding**: Integration function that combines WFST decoding with Neural LCS alignment\n",
    "\n",
    "The integration provides:\n",
    "- Better alignment accuracy through neural learning\n",
    "- Enhanced dysfluency detection capabilities\n",
    "- Flexible choice between hard and neural alignment methods\n",
    "- Comprehensive result reporting with alignment information\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
